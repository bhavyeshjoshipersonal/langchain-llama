import json
from typing import Literal
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.utils.function_calling import convert_to_openai_function

class GetCurrentWeather(BaseModel):
    """Get the current weather in a given location."""
    location: str = Field(description="The city and state, e.g. San Francisco, CA or Pune, Maharashtra")
    unit: Literal["celsius", "fahrenheit"] = Field(
        description="The unit of temperature to return", default="celsius"
    )

    def get_current_weather_function(location: str, unit: str = "celsius"):
        """
            Simulates fetching current weather data.
            In a real application, this would call an external API.
        """
        print(f"\n--- Calling get_current_weather_function for {location} in {unit} ---")
        # Mock data based on the city
        if "london" in location.lower():
            temp = "18" if unit == "celsius" else "64"
            return f"It's {temp}°{unit[0].upper()} and partly cloudy in London."
        elif "pune" in location.lower():
            # Current location is Pune, so let's make it realistic for June!
            temp = "30" if unit == "celsius" else "86"
            return f"It's {temp}°{unit[0].upper()} and rainy with moderate humidity in Pune."
        elif "new york" in location.lower():
            temp = "25" if unit == "celsius" else "77"
            return f"It's {temp}°{unit[0].upper()} and sunny in New York."
        else:
            return "Weather data not available for that location."

llm = ChatOllama(model="llama3.1", temperature=0)
tools_openai_format = [convert_to_openai_function(GetCurrentWeather)]
llm_with_tools = llm.bind_tools(tools_openai_format)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful AI assistant. If you need to know the weather, use the provided tool."),
        ("human", "{input}"),
    ]
)

def tool_executor(message):
    """
    Executes the tool call generated by the LLM.
    This function acts as a router.
    """
    if not message.tool_calls:
        return message.content # No tool call, return LLM's direct answer
    
    tool_outputs = []
    for tool_call in message.tool_calls:
        if tool_call.name == "get_current_weather":
            # Extract arguments and call our actual Python function
            tool_output = get_current_weather_function(**tool_call.args)
            tool_outputs.append(tool_output)
        # Add more `elif` blocks here for other tools if you have them
        else:
            tool_outputs.append(f"Error: Tool '{tool_call.name}' not found.")
            
    # For simplicity, we'll just return the first tool output if multiple were called.
    # In a real app, you might iterate or combine.
    return tool_outputs[0] if tool_outputs else ""

chain = prompt | llm_with_tools | tool_executor
print("--- Scenario 1: User asks for weather ---")
user_input_1 = "What's the weather like in Pune, Maharashtra today?"
print(f"User: {user_input_1}")
response_1 = chain.invoke({"input": user_input_1})
print(f"AI: {response_1}\n")